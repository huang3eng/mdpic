

## 常用链接

- ocr新算子仓库：https://gitlab.4pd.io/zhanghui/ocr-engine/-/tree/develop
- 服务器申请：https://ops-paas.4paradigm.com/#/order/3980
- 服务器列表：https://wiki.4paradigm.com/pages/viewpage.action?pageId=116165288
- jumpserver使用：https://wiki.4paradigm.com/pages/viewpage.action?pageId=106570705
- 查找python包地址：http://pypi.4paradigm.com/4paradigm/dev
- 查找whl包地址：https://nexus.4pd.io/#browse/search
- 上传文件：http://pkg.4paradigm.com:81/iRM/OCR/260debug/
- dali文档：https://docs.nvidia.com/deeplearning/dali/user-guide/docs/installation.html
- 服务器
  - 172.27.231.33
  - 172.27.231.43

## 配置环境

- mac安装brew

  ```
  /bin/zsh -c "$(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)"
  
  JAVA_HOME=/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home
  
  PATH=$JAVA_HOME/bin:$PATH:.
  
  CLASS_PATH=$JAVA_HOME/lib
  
  export JAVA_HOME
  
  export PATH
  
  export CLASS_PATH
  ```

- 常用命令

  - [mac生成ssh](https://www.jianshu.com/p/bdee4a3d1aa7?u_atoken=001c26b9-90f7-48de-a7fd-21c97929588c&u_asession=011mki3pSXQLYyakJHtLrJ5xKPtJsqc0tb3K_668AX0lsrH1CNp0YOTtLoI47nfSeWX0KNBwm7Lovlpxjd_P_q4JsKWYrT3W_NKPr8w6oU7K_nICLtWpZu4V7WAN5X9pfXzdjoMV1y19BFQvaXcOyBfmBkFo3NEHBv0PZUm6pbxQU&u_asig=05G8KSPN3M7NzycV7ubsMhT5yq6DjpmlY0vtI9idj42aza3UMChVlixuJl5a3s1ix6krzAGrgoeMNLrKfdybmFnR-XDwk91JkWLhhDjrKKn4XGUzuWEcszl8Y8VcGpWvZ6--C2ywa0xlGcHOWKYyI0O8hS_SE7fcPP119AoBPZfRv9JS7q8ZD7Xtz2Ly-b0kmuyAKRFSVJkkdwVUnyHAIJzf9BvRFrIYJwtYouklTIMpmEyRK1no71tJ6h-Eig_52OqBR97QLsOYcZJeUxi-_JXu3h9VXwMyh6PgyDIVSG1W_ZKRBPqMVMjwm2ztJP4_rdYOT1Y8pPHlV9rWKzI_RyLvBSNgC2X2fHd9EpDc01DgRYl6prYF8dm8fzCoh3AI0wmWspDxyAEEo4kbsryBKb9Q&u_aref=O4u7rcZI3UlOrCUDiaWlyDDzflI%3D)

  ```sh
  # 跳机
  ssh -p 10022 huangliangmeng@system-jumpserver.4paradigm.com
  ssh -p 10022 'huangliangmeng@system-jumpserver.4paradigm.com' 
  ssh -p 10022 '黄亮猛@system-jumpserver.4paradigm.com' 
  
  ssh -T -D 63777 -p 10022 "黄亮猛@root@172.27.231.33@system-jumpserver.4paradigm.com"
  
  vi /root/.jupyter/jupyter_notebook_config.py
  
  Your identification has been saved in /Users/4paradigm/.ssh/id_rsa
  Your public key has been saved in /Users/4paradigm/.ssh/id_rsa.pub
  The key fingerprint is:
  SHA256:8cVRUdEBpc5cs4QcCoJ4Szy4xCepqmxmEvWPSI0/IhI huangliangmeng@4paradigm.com
  
  cd ~/.ssh
  ssh-keygen -t rsa -C "huangliangmeng@4paradigm.com"
  vim ~/.ssh/id_rsa.pub
  cat ~/.ssh/id_rsa.pub
  
  Host jumper
   hostname system-jumpserver.4paradigm.com
   user "黄亮猛"
   port 10022
   preferredauthentications publickey
   
   
  'argon2:$argon2id$v=19$m=10240,t=10,p=8$dD8d7cnc/X9/294JJU6mkg$1mG+QLE5mpboOyLY1jP2JxXD4ZB2jrQU0HLUKA1PDCA'
  
  conda install -c conda-forge/label/cf202003 nodejs
  
  http://172.27.231.43:40121/fallback-ingress/jruntime.sage/jruntime-cycle-14/lab/tree/test_cv_predictor_acc.ipynb
  
  
  docker run -it -e NVIDIA_VISIBLE_DEVICES=0 --net=host docker.4pd.io/env/release/hypc-v0.0.1/prophet/appocr-runtime-2110:pipe-45-commit-7449fae1 bash
  
  
  
  
   start_custom_socr --output_model_folder socr-model --model_path http://pkg.4paradigm.com:81/iRM/OCR/260debug/new_model.pickle
  
  ```
  
  - triton client demo：https://github.com/triton-inference-server/client/tree/main/src/python/examples
  - Triton Server 和 CUDA 的版本依赖表：https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html``

```sh

# (--name=hlm_commit-7449fae1)

# docker version < 19 
docker run  -it -e NVIDIA_VISIBLE_DEVICES=0 --net=host docker.4pd.io/env/release/hypc-v0.0.1/prophet/appocr-runtime-2110:pipe-45-commit-7449fae1 bash
# docker version >= 19
docker run -it --gpus=0 --net=host docker.4pd.io/env/release/hypc-v0.0.1/prophet/appocr-runtime-2110:pipe-45-commit-7449fae1 bash

docker run --name=hlm-fix-inference -it -e NVIDIA_VISIBLE_DEVICES=0 --net=host docker.4pd.io/env/feat/fix-inference-v1.0.0/prophet/appocr-runtime-2110:pipe-59-commit-b3653fc9 bash

mkdir -p /models 

cd /models

wget http://pkg.4paradigm.com:81/iRM/OCR/operator/op_0_0_1/models0909/models-20220909-compute-capability-7.5.tar

tar -xf models-20220909-compute-capability-7.5.tar 

tritonserver --model-repository=/models


# docker run -it -e NVIDIA_VISIBLE_DEVICES=0 --net=host docker.4pd.io/env/release/hypc-v0.0.1/prophet/appocr-runtime-2110:pipe-45-commit-7449fae1 bash

```

- bug fix

  - https://gitlab.4pd.io/zhanghui/ocr-engine/-/commit/b3653fc9d4c983308926a3a8daed002418bdfe07

    ```
    tritonclient.utils.InferenceServerException: [StatusCode.INTERNAL] in ensemble 'ocr_ensemble_dali', Failed to process the request(s) for model instance 'intermediate_0_3', message: error: unpack_from requires a buffer of at least 3854474730 bytes for unpacking 3854474726 bytes at offset 4 (actual buffer size is 665)
    
    At:
      /opt/tritonserver/backends/python/triton_python_backend_utils.py(116): deserialize_bytes_tensor
      /usr/lib/python3.8/concurrent/futures/thread.py(63): run
      /usr/lib/python3.8/concurrent/futures/thread.py(103): _worker
      /usr/lib/python3.8/threading.py(874): run
      /usr/lib/python3.8/threading.py(942): _bootstrap_inner
      /usr/lib/python3.8/threading.py(894): _bootstrap
    ```

    

- 文件转移

```sh
# 本地文件转移到docker镜像内
docker cp /root/hlm/libtriton_rec_post.so db5066181108:/models/rec_post/1/

scp /Users/4paradigm/Downloads/libtriton_rec_post.so root@172.27.231.43:/root/
scp /path/filename username@servername:/path   
```

- 172.27.231.43上docker

```shell
docker run -it -e NVIDIA_VISIBLE_DEVICES=0 --rm --net=host docker.4pd.io/env/feat/hypc-v0.0.1/hypc/ocr/app/ocr-runtime-2110:pipe-142-commit-5db241c2 bash

docker run -it -e NVIDIA_VISIBLE_DEVICES=0 --rm --net=host -p 8506:8506 hypc-v0.0.1:modified bash

start_custom_socr --output_model_folder socr-model --model_path http://pkg.4paradigm.com:81/iRM/OCR/260debug/socr-model.zip

test_socr.py --url http://172.27.231.43:8506/lab/ocr/predict/ticket --scene platop --output /Users/4paradigm/projects/python/hypc-ocr-notebook/hctool/hctool/output


docker.4pd.io/env/feat/hypc-v0.0.1/hypc/ocr/app/ocr-runtime-2110:pipe-156-commit-ca2f4bbb
```

```
docker run -it -e NVIDIA_VISIBLE_DEVICES=0 --rm --net=host -p 8506:8506 docker.4pd.io/env/feat/hypc-v0.0.1/hypc/ocr/app/ocr-runtime-2110:pipe-156-commit-ca2f4bbb bash

start_custom_socr --output_model_folder socr-model --model_path http://pkg.4paradigm.com:81/iRM/OCR/260debug/socr-model.zip

```

- 172.27.231.43上k8s

```shell

tritonserver --model-repository=/models --backend-config=python,shm-default-byte-size=8388608

python3 -m site 

start_custom_socr --output_model_folder socr-model --model_path http://pkg.4paradigm.com:81/iRM/OCR/260debug/socr-model.zip --ocr-host localhost --ocr-port 8000

/usr/local/lib/python3.8/dist-packages/mlserver/predictor

/root/nn-predictor-source/models/socr_models/socr/shipinjinying/ocr.yaml


python3 start_socr.py --socr_data /root/nn-predictor-source/models/socr_models/ --target_socr_data /root/test --ocr-port=8000

# general model
python test_socr.py --url http://172.27.231.43:31241/lab/ocr/predict/general --scene chinese_print


python3 start_socr.py --socr_data /root/nn-predictor-source/models/socr_models/ --target_socr_data /root/test --ocr-port=8000

```

```shell
docker.4pd.io/env/feat/hypc-v0.0.1/hypc/ocr/app/ocr-runtime-2110:pipe-208-commit-29b41896

docker run -it -e NVIDIA_VISIBLE_DEVICES=0 --rm --net=host -p 8502:8502 -p 8506:8506 docker.4pd.io/env/feat/hypc-v0.0.1/hypc/ocr/app/ocr-runtime-2110:pipe-208-commit-29b41896 bash

docker.4pd.io/env/feat/hypc-v0.0.1/hypc/ocr/app/ocr-runtime-2110:pipe-234-commit-76a3f0a2

docker run -it -e NVIDIA_VISIBLE_DEVICES=0 --rm --net=host -p 8005:8000 docker.4pd.io/env/fix/edit-det-train-v0.0.1/hypc/ocr/app/ocr-runtime-2110:pipe-252-commit-23bb0a99 bash

python3 start_socr.py --target_socr_data /root/test --ocr-port=8000

python3 start_socr.py --target_socr_data /root/test --ocr-port=8000

 python ./hctool/test_socr.py --url http://172.27.231.43:31241/lab/ocr/predict/general --scene highspeed --output ./images_output

```

- table test

```shell
python test_general_socr.py --url http://172.27.231.31:32063/lab/ocr/predict/table --det-model mrcnn_v5_jit --recog-model transformer-blank-polished --general-model tablenetIns --scene table_print --output ./image_output_table

python test_general_socr.py --url http://172.27.231.31:32063/lab/ocr/predict/table --det-model mrcnn_v5_jit --recog-model transformer-blank-polished --general-model tablenetIns --scene table_print --output ./image_output_table
```

- license_agent

```shell
{
     "exec-opts":["native.cgroupdriver=systemd"],
     "registry-mirrors":["https://6kx4zyno.mirror.aliyuncs.com"],
     "log-driver":"json-file",
     "log-opts":{
         "max-size":"100m"
     }
 }
```

```shell
python -m tf2onnx.convert --graphdef /Users/4paradigm/PycharmProjects/demo/model/model.pb --output /Users/4paradigm/PycharmProjects/demo/model/model.onnx --inputs model/ImageTensor:0,model_1/ImageTensor:0 --outputs model/SemanticPredictions:0,model_1/SemanticPredictions:0


tritonserver --model-repository=/models --strict-model-config=false

tritonserver --model-repository=/models --strict-model-config=false &

docker run -it -e --rm --net=host -p8000:8000 -v/root/hlm/triton/rps:/models  nvcr.io/nvidia/tritonserver:21.10-py3 bash

docker run -it -e NVIDIA_VISIBLE_DEVICES=0 --rm -p8057:8000 -v/root/hlm/triton/rps:/models tritonserver-21-10:dali-cuda102 bash

```

```
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb

RUN apt-key del 7fa2af80 
RUN apt-get update && apt-get install -y --no-install-recommends wget
RUN wget https://developer.download.nvidia.com/compute/cuda/ubuntu2004/x86_64/x86_64/cuda-keyring_1.0-1_all.deb
RUN dpkg -i cuda-keyring_1.0-1_all.deb


 RUN apt-key del 7fa2af80 \
    && curl -L -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb \
    && dpkg -i cuda-keyring_1.0-1_all.deb
```

```
# copies or substantial portions of the Software.
 #
 # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
 # FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
 # COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
 # IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
 # CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

 # -------------------------------------------------- #
 # This Docker image presents an out-of-source build.
 # If you want a release build of dali_backend
 # inside the tritonserver and you don't want to build
 # the whole tritonserver, start from here.
 # -------------------------------------------------- #

 ARG TRITON_VERSION=21.10
 ARG BASE_IMAGE=nvcr.io/nvidia/tritonserver:${TRITON_VERSION}-py3
 FROM ${BASE_IMAGE} as builder


 RUN apt-key del 7fa2af80 \
     && curl -L -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb \
     && dpkg -i cuda-keyring_1.0-1_all.deb



 RUN apt-get update                                && \
     apt-get install -y software-properties-common && \
     add-apt-repository ppa:deadsnakes/ppa         && \
     apt-get update                                && \
     apt-get install -y         \
               zip              \
               wget             \
               build-essential  \
               autoconf         \
               autogen          \
               unzip            \
               python3.8        \
               python3-pip      \
               libboost-all-dev \
               rapidjson-dev && \
     rm -rf /var/lib/apt/lists/*

 # pip version in apt packages is ancient - we need to update it
 RUN pip3 install -U pip
 
 
 docker run -it -e --rm -p8057:8000 -v/root/hlm/triton/rps:/models tritonserver-21-10:dali-cuda102 bash
```

```
				rotate_radian = np.arctan2(y,x) #[-pi, pi]
        sign = 1 if rotate_radian >= 0 else -1

        rotate_radian = np.abs(rotate_radian)
        if 0 <= rotate_radian < np.pi / 4:
            rotate_radian = 0
        elif np.pi / 4 <= rotate_radian < np.pi / 2:
            rotate_radian = np.pi / 2
        elif np.pi / 2 <= rotate_radian < 3*np.pi / 4:
            rotate_radian = np.pi / 2
        else:
            rotate_radian = np.pi
        rotate_radian = rotate_radian * sign
        # 注意：这里把角度取反了。兼容新算法和老代码
        rotate_radian = -1 * rotate_radian
        rotate_angle = np.degrees(rotate_radian)
```

```
(1, 805, 1024, 5)
```

```
env/fix/fe-triton-client-v0.0.1/hypc/ocr/app/ocr-runtime-2110:pipe-296-commit-fb40279a
```

## table

- 如何使用dali
  - [在镜像中安装nvidia.dali（注意cuda版本）](https://docs.nvidia.com/deeplearning/dali/user-guide/docs/installation.html)
  - `python3 deserialize_model.py`生成对应的`model.dali`

- 172.27.69.29

```
docker run -it -e --rm -p8057:8000 -v/root/hlm/triton:/models docker.4pd.io/table-triton-server2110:dali-cuda102 bash

docker run -it -e --rm -p8057:8000 -v/root/hlm/triton:/models docker.4pd.io/table-triton-server2110:dali-cuda102-opencv bash

tritonserver --model-repository=/models/rps_three --strict-model-config=false

docker run -it -e --rm -p8057:8000 -v/root/hlm/triton:/models docker.4pd.io/ocr-runtime-base:22.09-py3 bash

```

- docker.4pd.io/table-triton-server2110:dali-cuda102-opencv

  ```
  docker run -it -e --rm -p8057:8000 -v/root/hlm/ocr-table-triton-v0.0.1:/models docker.4pd.io/ocr-runtime-base:22.09-py3-table bash
  
  tritonserver --model-repository=/models --backend-config=python,shm-default-byte-size=8388608 --log-verbose=1
  
  docker run -it -e --rm -p8057:8000 -v/root/hlm/ocr-table-triton-v0.0.1:/models docker.4pd.io/ocr-runtime-base:22.09-py3-table bash
  
  
  docker run -it -e --rm -p8057:8000 -v/root/hlm/triton_cls/rps_cls:/models docker.4pd.io/ocr-runtime-base:22.09-py3-table bash
  
  ```
  
  

## autocv

```
docker run -it -e --rm docker.4pd.io/autocv/autocv_running_env_cuda10:0.1 bash 

echo 'export PATH="/Users/4paradigm/opt/anaconda3/bin:$PATH"' >> ~/.zshrc

entry_points={
          'console_scripts': [
              'autocv_det_train=',

          ],
      },

```

- 问题
  - 缺失的包
    - .raccoon
      - .raccoon.raccoon.utils.config 
    - algo_cli
      - algo_cli.params
      - args
      - runner
  - 要把autocv做成一个可安装的python包吗

![image-20221027144726420](/Users/4paradigm/Library/Application Support/typora-user-images/image-20221027144726420.png)

```shell
autocv_det_train --batchsize -1 --complexity 2 --cpu_limit 12 --dataset /remote-data/DataSource/20223931-143946/0/hypcml_cyc1_sl3_train_image-group-label-1666787750.csv-20223931-143949/training_labels.csv --epoch -1.0 --gpu 2 --learning_rate -1.0 --memory_limit 3070Mi --warmup_iters -1.0 --warmup_ratio -1.0 --workdir /__operator_workdir__ --type 1

autocv_det_train --dataset /remote-data/DataSource/20223931-143946/0/hypcml_cyc1_sl3_train_image-group-label-1666787750.csv-20223931-143949/training_labels.csv --workdir hdfs:///user/prophet_172.27.231.34_4.1.0/workspace/xmz/pws/1/AUTO-CV-DAG-1-3/16/AUTO-CV-TRAIN


sh upload-to-pkg.sh http://pkg.4paradigm.com:81/iRM/OCR/dataset/debug/

bash run-autocv-with-gpu.sh docker.4pd.io/autocv/autocv_running_env_cuda11:0.7

# 安装了hypc-auto==1.0.0 tensorflow==2.3.0 下载了remote-data.tar.gz 
docker.4pd.io/autocv/autocv_running_env_cuda11:0.8

# 两个包的地址
http://pkg.4paradigm.com:81/iRM/OCR/dataset/debug/


 Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
    fn(i, *args)
  File "/root/miniconda3/lib/python3.8/site-packages/autocv/lib/functional/ddp_spawn.py", line 33, in _distributed_worker
    raise e
  File "/root/miniconda3/lib/python3.8/site-packages/autocv/lib/functional/ddp_spawn.py", line 21, in _distributed_worker
    dist.init_process_group(backend='nccl',
  File "/root/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 455, in init_process_group
    barrier()
  File "/root/miniconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1960, in barrier
    work = _default_pg.barrier()
RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:784, invalid usage, NCCL version 2.7.8


autocv_det_train --batchsize -1 --complexity 2 --cpu_limit 12 --dataset /remote-data/DataSource/20223931-143946/0/hypcml_cyc1_sl3_train_image-group-label-1666787750.csv-20223931-143949/training_labels.csv --epoch -1.0 --gpu 1 --learning_rate -1.0 --memory_limit 3070Mi --warmup_iters -1.0 --warmup_ratio -1.0 --workdir /__operator_workdir__ --type 1

```

```
wget http://pkg.4paradigm.com:81/iRM/OCR/dataset/debug/remote-data.tar.gz
tar -xzvf ./remote-data.tar.gz

autocv_det_train --batchsize -1 --complexity 2 --cpu_limit 12 --dataset /root/detection_cv210/labels.csv --epoch -1.0 --gpu 1 --learning_rate -1.0 --memory_limit 3070Mi --warmup_iters -1.0 --warmup_ratio -1.0 --workdir /__operator_workdir__ --type 1
```

```
autocv_cls_train --dataset_path /remote-data/classification_dataset/labels.csv --workdir /__cls_workdir__

searching_policies=['autoaug_train', 'character', 'scence', 'object', 'CT', 'pose', 'train', 'rand_augment']


```

```shell
autocv_cls_train --dataset_path /remote-data/valid_classification-0/labels.csv --workdir /__cls_workdir__ --input_size 448 --aug_search_epochs 5 --aug_search_lr_step 10 --epochs 6 --lr_search_steps 11


LR find stopped at 11th steps
All networks best LR found, [['resnest50', 0.000164483306164392], ['resnest101', 0.000164483306164392]]
Training all networks ...

All networks training done:
[
   [
      "resnest50",
      {
         "metrics":{
            "train":{
               "loss":[
                  0.5857483148574829,
                  0.5777525305747986,
                  0.581749677658081,
                  0.5550037622451782,
                  0.5228495001792908,
                  0.5783312916755676
               ],
               "acc":[
                  75.0,
                  83.33332824707031,
                  91.66666412353516,
                  83.33332824707031,
                  75.0,
                  66.66666412353516
               ]
            }
         },
         "saved_model_path":"/__cls_workdir__/formal_train/formal_train/resnest50/ckpt/model_final.pth"
      }
   ],
   [
      "resnest101",
      {
         "metrics":{
            "train":{
               "loss":[
                  0.6702620983123779,
                  0.6423112750053406,
                  0.7723411321640015,
                  0.6044177412986755,
                  0.6223816871643066,
                  0.6047269701957703
               ],
               "acc":[
                  75.0,
                  66.66666412353516,
                  33.33333206176758,
                  75.0,
                  66.66666412353516,
                  66.66666412353516
               ]
            }
         },
         "saved_model_path":"/__cls_workdir__/formal_train/formal_train/resnest101/ckpt/model_final.pth"
      }
   ]
]

```

```
/root/miniconda3/lib/python3.8/site-packages/autocv/cli/utils/utils.py(91)process_dataset()
/root/miniconda3/lib/python3.8/site-packages/autocv/lib/datasets/classify/utils.py(19)get_split_num()
/root/miniconda3/lib/python3.8/site-packages/autocv/lib/datasets/classify/utils.py(75)split_k_fold()
```

```
服务器172.27.231.100
两个正在运行的docker镜像
docker attach 7496abd8b50b
docker attach db6a789a032e

1.删除上一次训练的工作目录：rm -rf /__cls_workdir__/
2.开启新的训练：autocv_cls_train --dataset_path /remote-data/valid_classification-0/labels.csv --workdir /__cls_workdir__  --input_size 448  --epochs 4 --lr_search_steps 5 --aug_policy train --gpu 2

vim /root/miniconda3/lib/python3.8/site-packages/autocv/raccoon/raccoon/utils/checkpoint.py
/root/miniconda3/lib/python3.8/site-packages/autocv/classify/policy/functions

```

```shell
k get ns

k run autocv-test --image=docker.4pd.io/autocv/autocv_running_env_cuda11:0.9 --namespace=prophet-resource-autocv

k -n prophet-resource-autocv describe pod <podname>

k -n prophet-resource-autocv logs <podname>

k -n prophet-resource-autocv get pods
k -n prophet-resource-autocv delete pod <pod_name>

k -n prophet-resource-autocv get deployment
k -n prophet-resource-autocv delete deployment <deployment_name>

k apply -f autocv_deployment.yaml
# 进入k8s容器中
k exec -it -n prophet-resource-autocv autocv-deployment-5b8cf8577b-jg6jc -- bash

# k8s pod中多进程通信使用共享内存，共享内存不足报错
RuntimeError: DataLoader worker (pid 2479) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.

# k8s的端口转发，如果不指定address，默认转发的主机是127.0.0.1
k port-forward -n prophet-resource-autocv  autocv-deployment-6cc99cf744-5cx5z --address 0.0.0.0 8080:8080
```



```shell
curl -X POST -F 'data=@./1.jpeg'  http://172.27.231.43:8080
```

```
tiup cluster deploy tidb-cluster-demo v6.5.0 ./topo.yaml --ssh none --user root -p

tiup cluster deploy tidb-cluster-demo v6.5.0 ./topo.yaml --ssh none --user root -p

tiup cluster check ./topo.yaml --apply --ssh none --user root -p
```

```shell
lsblk 
sudo mount --bind /mnt/disk4/ti/ssd /mnt/ti/ssd
sudo mount --bind /mnt/disk4/ti/backup /mnt/ti/backup
sudo mount --bind /mnt/disk4/ti/monitoring /mnt/ti/monitoring
sudo mount --bind /mnt/disk4/ti/sharedssd /mnt/ti/sharedssd

sudo mount --bind /mnt/disk4/ti/local /mnt/ti/local &&
sudo mount --bind /mnt/disk4/ti/local1 /mnt/ti/local1 &&
sudo mount --bind /mnt/disk4/ti/local2 /mnt/ti/local2 &&
sudo mount --bind /mnt/disk4/ti/local3 /mnt/ti/local3 &&
sudo mount --bind /mnt/disk4/ti/local4 /mnt/ti/local4 

mount --bind /mnt/disk4/ti/ssd /mnt/ti/ssd
echo /mnt/disk4/ti/ssd /mnt/ti/ssd none bind 0 0 | sudo tee -a /etc/fstab
echo /mnt/disk4/ti/backup /mnt/ti/backup none bind 0 0 | sudo tee -a /etc/fstab
echo /mnt/disk4/ti/monitoring /mnt/ti/monitoring none 0 0 | sudo tee -a /etc/fstab
echo /mnt/disk4/ti/sharedssd /mnt/ti/sharedssd none bind 0 0 | sudo tee -a /etc/fstab

helm install tidb-operator pingcap/tidb-operator --namespace=tidb-admin --version=v1.4.0 -f ${HOME}/tidb-operator/values-tidb-operator.yaml && \
kubectl get po -n tidb-admin -l app.kubernetes.io/name=tidb-operator

https://help.aliyun.com/document_detail/132705.html

docker tag bitnami/kubectl:latest docker.4pd.io/bitnami/kubectl:latest
docker push docker.4pd.io/bitnami/kubectl:latest
```

```
docker pull docker.4pd.io/pingcap/tidb-operator:v1.4.0 &&
docker pull docker.4pd.io/pingcap/tidb-backup-manager:v1.4.0 &&
docker pull docker.4pd.io/bitnami/kubectl:latest &&
docker pull docker.4pd.io/pingcap/advanced-statefulset:v0.3.3

docker pull docker.4pd.io/pingcap/pd:v6.5.0 &&
docker pull docker.4pd.io/pingcap/tikv:v6.5.0 &&
docker pull docker.4pd.io/pingcap/tidb:v6.5.0 &&
docker pull docker.4pd.io/pingcap/tidb-binlog:v6.5.0 &&
docker pull docker.4pd.io/pingcap/ticdc:v6.5.0 &&
docker pull docker.4pd.io/pingcap/tiflash:v6.5.0 &&
docker pull docker.4pd.io/pingcap/tidb-monitor-reloader:v1.0.1 &&
docker pull docker.4pd.io/pingcap/tidb-monitor-initializer:v6.5.0 &&
docker pull docker.4pd.io/grafana/grafana:6.0.1 &&
docker pull docker.4pd.io/prom/prometheus:v2.18.1 &&
docker pull docker.4pd.io/busybox:1.26.2
```

```
k -n kube-system get ds
k -n kube-system get cm
k -n kube-system get cm local-provisioner-config -o yaml
k -n kube-system edit cm/local-provisioner-config 
k get pv
k -n namespace get pvc
```



```
k -n tidb-cluster apply -f ./tidb-cluster/tidb-initializer.yaml
wget http://pkg.4paradigm.com/iRM/OCR/dataset/debug/tidb-cluster.yaml

advanced-tidb-ticdc advanced-tidb-pump advanced-tidb-tiflash
```

